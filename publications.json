{
  "publications": [
    {
      "id": "sos",
      "title": "SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding",
      "authors": ["Weikai Huang", "Jieyu Zhang", "Taoyang Jia", "Chenhao Zheng", "Ziqi Gao", "Jae Sung Park", "Ranjay Krishna"],
      "venue": "arXiv",
      "year": "2025",
      "image": "img/papers/sos.png",
      "links": {
        "website": "https://github.com/weikaih04/Synthetic-Detection-Segmentation-Grounding-Data",
        "code": "https://github.com/weikaih04/Synthetic-Detection-Segmentation-Grounding-Data",
        "huggingface": "https://huggingface.co/collections/weikaih/sos-synthetic-object-segments-improves-detection-segmentat-682679751d20faa20800033c"
      },
      "selected": true,
      "tldr": "20M synthetic object segments—boost GroundingDINO, Mask2Former on LVIS, gRefCOCO. First synthetic data surpassing real datasets in low-data regimes."
    },
    {
      "id": "gas",
      "title": "Generate Any Scene: Scene Graph Driven Data Synthesis for Visual Generation Training",
      "authors": ["Ziqi Gao*", "Weikai Huang*", "Jieyu Zhang", "Aniruddha Kembhavi", "Ranjay Krishna"],
      "venue": "arXiv",
      "year": "2025",
      "image": "img/papers/gas.png",
      "links": {
        "website": "https://generate-any-scene.github.io/",
        "arxiv": "https://arxiv.org/abs/2412.08221",
        "code": "https://github.com/generate-any-scene/generate-any-scene"
      },
      "selected": true,
      "tldr": "Scene graph programming generates infinite compositional scenes—systematically improves text-to-image, text-to-video and text-to-3D models. Also support RL training!"
    },
    {
      "id": "provision",
      "title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models",
      "authors": ["Jieyu Zhang", "Le Xue", "Linxin Song", "Jun Wang", "Weikai Huang", "Manli Shu", "An Yan", "Zixian Ma", "Juan Carlos Niebles", "Silvio Savarese", "Caiming Xiong", "Zeyuan Chen", "Ranjay Krishna", "Ran Xu"],
      "venue": "arXiv",
      "year": "2024",
      "image": "img/papers/provision.png",
      "links": {
        "arxiv": "https://arxiv.org/abs/2412.07012",
        "code": "https://github.com/JieyuZ2/ProVision",
        "huggingface": "https://huggingface.co/datasets/Salesforce/ProVision-10M",
        "blog": "https://www.salesforce.com/blog/provision-multimodal-data-generation/"
      },
      "selected": true,
      "tldr": "10M vision-centric VQA questions generated by 100% accurate program—7% boost on CVBench 2D, 8% on 3D, 8% on Mantis-Eval. Zero manual annotation."
    },
    {
      "id": "tma",
      "title": "Task Me Anything",
      "authors": ["Jieyu Zhang", "Weikai Huang*", "Zixian Ma*", "Oscar Michel", "Dong He", "Tanmay Gupta", "Wei-Chiu Ma", "Ali Farhadi", "Aniruddha Kembhavi", "Ranjay Krishna"],
      "venue": "NeurIPS (Main Conference)",
      "year": "2024",
      "award": null,
      "additional_venue": "Video-Language Models Workshop (Oral)",
      "image": "img/papers/tma.png",
      "links": {
        "arxiv": "https://arxiv.org/abs/2406.11775",
        "website": "https://www.task-me-anything.org/",
        "code": "https://github.com/JieyuZ2/TaskMeAnything",
        "huggingface": "https://huggingface.co/collections/jieyuz2/taskme-anything-667e1f31bdfedf6c0c3b2e03",
        "blog": "https://snorkel.ai/blog/task-me-anything-innovating-multimodal-model-benchmarks/",
        "talk": "https://www.youtube.com/watch?v=J3ECnV8Yc_g"
      },
      "selected": true,
      "tldr": "Provided 2D/3D assets, generate 750M+ fine-grained visual questions without any human annotation—reveals SOTA VLM's weaknesses."
    },
    {
      "id": "mms",
      "title": "m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks",
      "authors": ["Zixian Ma", "Weikai Huang", "Jieyu Zhang", "Tanmay Gupta", "Ranjay Krishna"],
      "venue": "ECCV",
      "year": "2024",
      "image": "img/papers/mms.png",
      "links": {
        "arxiv": "https://arxiv.org/abs/2403.11085",
        "website": "https://mnms-project.github.io/",
        "code": "https://github.com/RAIVNLab/mnms"
      },
      "selected": true,
      "tldr": "First multi-modal tool-use benchmark—4K+ human-verified tasks with 33 tools, GPT-4V achieves only 24% success rate."
    }
  ]
}

