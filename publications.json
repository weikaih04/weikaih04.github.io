{
  "publications": [
    {
      "id": "sos",
      "title": "SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding",
      "authors": [
        "Weikai Huang",
        "Jieyu Zhang",
        "Taoyang Jia",
        "Chenhao Zheng",
        "Ziqi Gao",
        "Jae Sung Park",
        "Ranjay Krishna"
      ],
      "venue": "arXiv",
      "year": "2025",
      "image": "img/papers/sos.png",
      "arxiv_id": "2510.09110",
      "github_repo": "weikaih04/Synthetic-Detection-Segmentation-Grounding-Data",
      "links": {
        "arxiv": "https://arxiv.org/abs/2510.09110",
        "website": "https://github.com/weikaih04/Synthetic-Detection-Segmentation-Grounding-Data",
        "code": "https://github.com/weikaih04/Synthetic-Detection-Segmentation-Grounding-Data",
        "huggingface": "https://huggingface.co/collections/weikaih/sos-synthetic-object-segments-improves-detection-segmentat-682679751d20faa20800033c"
      },
      "selected": true,
      "tldr": "1M diverse and dense-annotated synthetic images composed by 20M synthetic objectâ€”boost GroundingDINO, Mask2Former on LVIS, gRefCOCO. First synthetic dataset surpassing real datasets in different models / benchmarks.",
      "github_stars": 63,
      "citations": 0
    },
    {
      "id": "gas",
      "title": "Generate Any Scene: Scene Graph Driven Data Synthesis for Visual Generation Training",
      "authors": [
        "Ziqi Gao*",
        "Weikai Huang*",
        "Jieyu Zhang",
        "Aniruddha Kembhavi",
        "Ranjay Krishna"
      ],
      "venue": "arXiv",
      "year": "2025",
      "image": "img/papers/gas.png",
      "arxiv_id": "2412.08221",
      "github_repo": "RAIVNLab/GenerateAnyScene",
      "links": {
        "website": "https://generate-any-scene.github.io/",
        "arxiv": "https://arxiv.org/abs/2412.08221",
        "code": "https://github.com/RAIVNLab/GenerateAnyScene",
        "huggingface": "https://huggingface.co/datasets/UWGZQ/GenerateAnyScene"
      },
      "selected": true,
      "tldr": "Scene graph programming generates infinite compositional scenesâ€”systematically improves text-to-image, text-to-video and text-to-3D models. Also support RL training!",
      "github_stars": 96,
      "citations": 0
    },
    {
      "id": "provision",
      "title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models",
      "authors": [
        "Jieyu Zhang",
        "Le Xue",
        "Linxin Song",
        "Jun Wang",
        "Weikai Huang",
        "Manli Shu",
        "An Yan",
        "Zixian Ma",
        "Juan Carlos Niebles",
        "Silvio Savarese",
        "Caiming Xiong",
        "Zeyuan Chen",
        "Ranjay Krishna",
        "Ran Xu"
      ],
      "venue": "arXiv",
      "year": "2024",
      "image": "img/papers/provision.png",
      "arxiv_id": "2412.07012",
      "github_repo": "JieyuZ2/ProVision",
      "links": {
        "arxiv": "https://arxiv.org/abs/2412.07012",
        "code": "https://github.com/JieyuZ2/ProVision",
        "huggingface": "https://huggingface.co/datasets/Salesforce/ProVision-10M",
        "blog": "https://www.salesforce.com/blog/provision-multimodal-data-generation/"
      },
      "selected": true,
      "tldr": "10M vision-centric VQA questions generated by 100% accurate programâ€”7% boost on CVBench 2D, 8% on 3D, 8% on Mantis-Eval. Zero manual annotation.",
      "highlight": "ðŸ”¥ Used in Salesforce's SOTA VLM <a href='https://github.com/salesforce/LAVIS/tree/xgen-mm' target='_blank' style='color: #ffffff; text-decoration: underline;'>BLIP-3</a> training mixture | ðŸ“° Covered by <a href='https://venturebeat.com/data-infrastructure/breaking-the-data-bottleneck-salesforces-provision-speeds-multimodal-ai-training-with-image-scene-graphs' target='_blank' style='color: #ffffff; text-decoration: underline;'>VentureBeat</a> & <a href='https://www.marktechpost.com/2024/12/10/salesforce-ai-research-introduces-provision-a-programmatic-approach-to-scale-vision-centric-instruction-data-for-multimodal-language-models/' target='_blank' style='color: #ffffff; text-decoration: underline;'>MarkTechPost</a>",
      "github_stars": 35,
      "citations": 11
    },
    {
      "id": "tma",
      "title": "Task Me Anything",
      "authors": [
        "Jieyu Zhang",
        "Weikai Huang*",
        "Zixian Ma*",
        "Oscar Michel",
        "Dong He",
        "Tanmay Gupta",
        "Wei-Chiu Ma",
        "Ali Farhadi",
        "Aniruddha Kembhavi",
        "Ranjay Krishna"
      ],
      "venue": "NeurIPS (Main Conference)",
      "year": "2024",
      "award": null,
      "additional_venue": "Video-Language Models Workshop (Oral)",
      "image": "img/papers/tma.png",
      "arxiv_id": "2406.11775",
      "github_repo": "JieyuZ2/TaskMeAnything",
      "links": {
        "arxiv": "https://arxiv.org/abs/2406.11775",
        "website": "https://www.task-me-anything.org/",
        "code": "https://github.com/JieyuZ2/TaskMeAnything",
        "huggingface": "https://huggingface.co/collections/jieyuz2/taskme-anything-667e1f31bdfedf6c0c3b2e03",
        "blog": "https://snorkel.ai/blog/task-me-anything-innovating-multimodal-model-benchmarks/",
        "talk": "https://www.youtube.com/watch?v=J3ECnV8Yc_g"
      },
      "selected": true,
      "tldr": "Provided 2D/3D assets, generate 750M+ fine-grained visual questions without any human annotationâ€”reveals SOTA VLM's weaknesses.",
      "highlight": "ðŸ“° Covered by <a href='https://snorkel.ai/blog/task-me-anything-innovating-multimodal-model-benchmarks/' target='_blank' style='color: #ffffff; text-decoration: underline;'>Snorkel AI</a> | ðŸŽ¥ Featured <a href='https://www.youtube.com/watch?v=J3ECnV8Yc_g' target='_blank' style='color: #ffffff; text-decoration: underline;'>Talk</a>",
      "github_stars": 73,
      "citations": 22
    },
    {
      "id": "mms",
      "title": "m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks",
      "authors": [
        "Zixian Ma",
        "Weikai Huang",
        "Jieyu Zhang",
        "Tanmay Gupta",
        "Ranjay Krishna"
      ],
      "venue": "ECCV",
      "year": "2024",
      "image": "img/papers/mms.png",
      "arxiv_id": "2403.11085",
      "github_repo": "RAIVNLab/mnms",
      "links": {
        "arxiv": "https://arxiv.org/abs/2403.11085",
        "website": "https://mnms-project.github.io/",
        "code": "https://github.com/RAIVNLab/mnms",
        "huggingface": "https://huggingface.co/datasets/zixianma/mnms"
      },
      "selected": true,
      "tldr": "First multi-modal tool-use benchmarkâ€”4K+ human-verified tasks with 33 tools, GPT-4V achieves only 24% success rate.",
      "github_stars": 43,
      "citations": 30
    }
  ]
}